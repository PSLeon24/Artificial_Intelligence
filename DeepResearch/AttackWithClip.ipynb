{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "147ab2f6-5486-4987-933b-b5f7a65a9f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# CLIP 모델 및 텍스트 설명 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 블러링을 유도할 텍스트 설명\n",
    "texts = [\"eyes\", \"nose\", \"mouth\"]\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "def generate_mild_noise(image, clip_model, text_tokens, epsilon=0.001, steps=100):\n",
    "    # 이미지 전처리\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "    noise = torch.zeros_like(image_tensor, requires_grad=True).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam([noise], lr=epsilon)\n",
    "\n",
    "    for step in range(steps):\n",
    "        perturbed_image = image_tensor + noise\n",
    "\n",
    "        # CLIP 모델로 이미지 임베딩 계산\n",
    "        perturbed_image_normalized = (perturbed_image - perturbed_image.min()) / (perturbed_image.max() - perturbed_image.min())\n",
    "        clip_image_embeds = clip_model.encode_image(perturbed_image_normalized)\n",
    "\n",
    "        # CLIP 텍스트 임베딩 계산 및 유사도 측정\n",
    "        similarity = torch.mean(clip_model.encode_text(text_tokens) @ clip_image_embeds.T)\n",
    "\n",
    "        # 손실 함수: 텍스트 설명과 이미지 유사도를 최소화\n",
    "        loss = -similarity\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 노이즈 클리핑 (아주 미세한 값으로 유지)\n",
    "        noise.data = torch.clamp(noise.data, -epsilon, epsilon)\n",
    "\n",
    "    return (image_tensor + noise).squeeze().detach().cpu().numpy()\n",
    "\n",
    "# 예시 사용\n",
    "image_path = 'yeongmin.jpeg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# 미세한 노이즈를 통한 블러링 유도\n",
    "noisy_image = generate_mild_noise(image, model, text_tokens)\n",
    "\n",
    "# 결과 이미지 저장\n",
    "noisy_image = np.clip(noisy_image, 0, 255).astype(np.uint8)\n",
    "noisy_image = np.transpose(noisy_image, (1, 2, 0))\n",
    "cv2.imwrite('noisy_face_with_clip_effect.jpg', cv2.cvtColor(noisy_image, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b347694b-2719-46d3-822f-338d74cea927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted image saved at: fixed_noisy_face_with_clip_effect.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def adjust_image(image):\n",
    "    # Check if the image is loaded correctly\n",
    "    if image is None:\n",
    "        print(\"Error: Image not loaded properly.\")\n",
    "        return None\n",
    "    \n",
    "    # Normalize image to 0-255 range\n",
    "    norm_image = cv2.normalize(image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "    return norm_image.astype(np.uint8)\n",
    "\n",
    "# Try to load the original noisy image\n",
    "image_path = 'noisy_face_with_clip_effect.jpg'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# If the image wasn't loaded, print an error message\n",
    "if image is None:\n",
    "    print(\"The image could not be loaded. Please check the file path or use a different image.\")\n",
    "else:\n",
    "    # Adjust the image to make sure it's in a visible range\n",
    "    fixed_image = adjust_image(image)\n",
    "\n",
    "    # Save the adjusted image if it's successfully loaded\n",
    "    if fixed_image is not None:\n",
    "        output_path = 'fixed_noisy_face_with_clip_effect.jpg'\n",
    "        cv2.imwrite(output_path, fixed_image)\n",
    "        print(f\"Adjusted image saved at: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eee3b2d7-206b-41f6-951f-e8dccc8bffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined image saved at: combined_image_with_noise.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load the original image (you can replace this with the actual original image path)\n",
    "original_image_path = 'yeongmin.jpeg'\n",
    "original_image = cv2.imread(original_image_path)\n",
    "\n",
    "# Load the noisy image (the one you uploaded)\n",
    "noisy_image_path = 'fixed_noisy_face_with_clip_effect.jpg'\n",
    "noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "# Check if both images are loaded correctly\n",
    "if original_image is None or noisy_image is None:\n",
    "    print(\"Error loading images. Please check the file paths.\")\n",
    "else:\n",
    "    # Resize noisy image to match the original image size if they differ\n",
    "    if original_image.shape != noisy_image.shape:\n",
    "        noisy_image = cv2.resize(noisy_image, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Combine the images with a weighted sum (alpha controls the noise intensity)\n",
    "    alpha = 0.5  # You can adjust this value to control the strength of the noise\n",
    "    combined_image = cv2.addWeighted(original_image, 1.0, noisy_image, alpha, 0)\n",
    "\n",
    "    # Save the combined image\n",
    "    output_path = 'combined_image_with_noise.jpg'\n",
    "    cv2.imwrite(output_path, combined_image)\n",
    "\n",
    "    # Display the resulting image\n",
    "    combined_pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "    combined_pil_image.show()\n",
    "\n",
    "    print(f\"Combined image saved at: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "be8a8456-6ea0-49e0-a9ac-bbfeadd53d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined image saved at: combined_image_with_noise_fixed.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "import clip\n",
    "\n",
    "# CLIP 모델 및 텍스트 설명 로드\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "# 블러링을 유도할 텍스트 설명\n",
    "texts = [\"blurred eyes\", \"blurred nose\", \"blurred mouth\"]\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "def generate_mild_noise(image, clip_model, text_tokens, epsilon=0.01, steps=1000):\n",
    "    # 이미지 전처리\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "    noise = torch.zeros_like(image_tensor, requires_grad=True).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam([noise], lr=epsilon)\n",
    "\n",
    "    for step in range(steps):\n",
    "        perturbed_image = image_tensor + noise\n",
    "\n",
    "        # CLIP 모델로 이미지 임베딩 계산\n",
    "        perturbed_image_normalized = (perturbed_image - perturbed_image.min()) / (perturbed_image.max() - perturbed_image.min())\n",
    "        clip_image_embeds = clip_model.encode_image(perturbed_image_normalized)\n",
    "\n",
    "        # CLIP 텍스트 임베딩 계산 및 유사도 측정\n",
    "        similarity = torch.mean(clip_model.encode_text(text_tokens) @ clip_image_embeds.T)\n",
    "\n",
    "        # 손실 함수: 텍스트 설명과 이미지 유사도를 최소화\n",
    "        loss = -similarity\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 노이즈 클리핑 (아주 미세한 값으로 유지)\n",
    "        noise.data = torch.clamp(noise.data, -epsilon, epsilon)\n",
    "\n",
    "    # 이미지를 0-255 범위로 다시 조정\n",
    "    final_image = torch.clamp((image_tensor + noise) * 255, 0, 255).detach().cpu().numpy().squeeze()\n",
    "    final_image = final_image.transpose(1, 2, 0).astype(np.uint8)\n",
    "    \n",
    "    return final_image\n",
    "\n",
    "def combine_images(original_image_path, noisy_image_path, output_path, alpha=0.3):\n",
    "    # Load the original and noisy images\n",
    "    original_image = cv2.imread(original_image_path)\n",
    "    noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "    # Ensure both images are loaded correctly\n",
    "    if original_image is None or noisy_image is None:\n",
    "        print(\"Error loading images. Please check the file paths.\")\n",
    "        return\n",
    "\n",
    "    # Resize the noisy image to match the original image size if they differ\n",
    "    noisy_image = cv2.resize(noisy_image, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Combine the images with a weighted sum\n",
    "    combined_image = cv2.addWeighted(original_image, 1.0, noisy_image, alpha, 0)\n",
    "\n",
    "    # Normalize the combined image to ensure visibility\n",
    "    combined_image = cv2.normalize(combined_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Save the combined image\n",
    "    cv2.imwrite(output_path, combined_image)\n",
    "\n",
    "    # Display the resulting image\n",
    "    combined_pil_image = Image.fromarray(cv2.cvtColor(combined_image, cv2.COLOR_BGR2RGB))\n",
    "    combined_pil_image.show()\n",
    "\n",
    "    print(f\"Combined image saved at: {output_path}\")\n",
    "\n",
    "# Step 1: Generate and save the noisy image\n",
    "image_path = 'yeongmin.jpeg'  # Path to the original image\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "# Generate the noisy image using CLIP-based noise generation\n",
    "noisy_image = generate_mild_noise(image, clip_model, text_tokens)\n",
    "\n",
    "# Save the noisy image\n",
    "noisy_image_path = 'fixed_noisy_face_with_clip_effect.jpg'\n",
    "cv2.imwrite(noisy_image_path, noisy_image)\n",
    "\n",
    "# Step 2: Combine the original and noisy images\n",
    "output_path = 'combined_image_with_noise_fixed.jpg'\n",
    "combine_images(image_path, noisy_image_path, output_path, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "37d8a812-158d-4117-81f6-45981788cff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_image_difference(original_image_path, noisy_image_path, output_path):\n",
    "    # Load the original and noisy images\n",
    "    original_image = cv2.imread(original_image_path)\n",
    "    noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "    # Ensure both images are the same size\n",
    "    if original_image.shape != noisy_image.shape:\n",
    "        noisy_image = cv2.resize(noisy_image, (original_image.shape[1], original_image.shape[0]))\n",
    "\n",
    "    # Compute the absolute difference between the images\n",
    "    difference_image = cv2.absdiff(original_image, noisy_image)\n",
    "\n",
    "    # Enhance the difference image for better visualization\n",
    "    # Option 1: Scale up the differences for better visibility\n",
    "    difference_image = cv2.normalize(difference_image, None, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "    # Option 2: Apply a contrast enhancement (uncomment if needed)\n",
    "    # difference_image = cv2.convertScaleAbs(difference_image, alpha=3, beta=0)\n",
    "\n",
    "    # Save the difference image\n",
    "    cv2.imwrite(output_path, difference_image)\n",
    "\n",
    "    return difference_image\n",
    "\n",
    "# Usage Example\n",
    "original_image_path = 'yeongmin.jpeg'  # Path to the original image\n",
    "noisy_image_path = 'combined_image_with_noise_fixed.jpg'  # Path to the noisy image\n",
    "difference_image_path = 'difference_image_enhanced.jpg'  # Path to save the enhanced difference image\n",
    "\n",
    "difference_image = compute_image_difference(original_image_path, noisy_image_path, difference_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "303633ca-08f3-434f-91d4-3461aba3b9d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Classification Probabilities:\n",
      "a photo of a person: 99.80%\n",
      "blurred eyes: 0.09%\n",
      "blurred nose: 0.08%\n",
      "blurred mouth: 0.02%\n",
      "Noisy Image Classification Probabilities:\n",
      "a photo of a person: 99.17%\n",
      "blurred eyes: 0.45%\n",
      "blurred nose: 0.28%\n",
      "blurred mouth: 0.10%\n",
      "Difference Image Classification Probabilities:\n",
      "a photo of a person: 99.27%\n",
      "blurred eyes: 0.47%\n",
      "blurred nose: 0.19%\n",
      "blurred mouth: 0.06%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load CLIP model and preprocess function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Define the text descriptions you want to classify with\n",
    "texts = [\"a photo of a person\", \"blurred eyes\", \"blurred nose\", \"blurred mouth\"]\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "def compute_image_difference(original_image_path, noisy_image_path, output_path):\n",
    "    # Load the original and noisy images\n",
    "    original_image = cv2.imread(original_image_path)\n",
    "    noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "    # Compute the absolute difference between the images\n",
    "    difference_image = cv2.absdiff(original_image, noisy_image)\n",
    "\n",
    "    # Save the difference image\n",
    "    cv2.imwrite(output_path, difference_image)\n",
    "\n",
    "    # Return the difference image for further analysis\n",
    "    return difference_image\n",
    "\n",
    "def classify_image(image, model, text_tokens):\n",
    "    # Preprocess the image\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode the image and texts using CLIP\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # Calculate the similarity between the image and each text prompt\n",
    "    similarities = (image_features @ text_features.T).squeeze(0)\n",
    "    \n",
    "    # Normalize similarities to get probabilities (optional)\n",
    "    probs = similarities.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "def print_classification_results(probs, descriptions):\n",
    "    # Print each text description with its corresponding probability\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"{descriptions[i]}: {prob * 100:.2f}%\")\n",
    "\n",
    "# Step 1: Compute the difference image\n",
    "original_image_path = 'yeongmin.jpeg'  # Path to the original image\n",
    "noisy_image_path = 'combined_image_with_noise_fixed.jpg'  # Path to the combined noisy image\n",
    "difference_image_path = 'difference_image.jpg'  # Path to save the difference image\n",
    "\n",
    "difference_image = compute_image_difference(original_image_path, noisy_image_path, difference_image_path)\n",
    "\n",
    "# Step 2: Classify the original, noisy, and difference images\n",
    "original_image = cv2.imread(original_image_path)\n",
    "noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "# Classify the original image\n",
    "original_probs = classify_image(original_image, model, text_tokens)\n",
    "print(\"Original Image Classification Probabilities:\")\n",
    "print_classification_results(original_probs, texts)\n",
    "\n",
    "# Classify the noisy image\n",
    "noisy_probs = classify_image(noisy_image, model, text_tokens)\n",
    "print(\"Noisy Image Classification Probabilities:\")\n",
    "print_classification_results(noisy_probs, texts)\n",
    "\n",
    "# Optional: Classify the difference image to see how the noise is perceived\n",
    "difference_probs = classify_image(difference_image, model, text_tokens)\n",
    "print(\"Difference Image Classification Probabilities:\")\n",
    "print_classification_results(difference_probs, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8f079b10-f898-423c-b554-cfc2a738c0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "# Load CLIP model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "def generate_adversarial_noise(image_tensor, model, target_text_token, epsilon=0.03, steps=10):\n",
    "    image_tensor = image_tensor.clone().detach().requires_grad_(True).to(device)\n",
    "    target_text_token = target_text_token.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([image_tensor], lr=epsilon)\n",
    "\n",
    "    for step in range(steps):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Normalize the image tensor\n",
    "        normalized_image = (image_tensor - image_tensor.min()) / (image_tensor.max() - image_tensor.min())\n",
    "        \n",
    "        # Forward pass\n",
    "        image_features = model.encode_image(normalized_image)\n",
    "        text_features = model.encode_text(target_text_token)\n",
    "        \n",
    "        # Compute loss (maximize similarity)\n",
    "        similarity = (image_features @ text_features.T).squeeze(0)\n",
    "        loss = -similarity.mean()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Clamp the perturbation to the epsilon level\n",
    "        perturbation = torch.clamp(image_tensor - image_tensor.detach(), -epsilon, epsilon)\n",
    "        image_tensor = torch.clamp(image_tensor + perturbation, 0, 1).detach().requires_grad_(True)\n",
    "\n",
    "    return image_tensor\n",
    "\n",
    "def visualize_perturbation(original_image, adversarial_image):\n",
    "    perturbation = adversarial_image - original_image\n",
    "    perturbation = perturbation.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    perturbation = (perturbation - perturbation.min()) / (perturbation.max() - perturbation.min())\n",
    "    \n",
    "    adversarial_image = adversarial_image.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    original_image = original_image.cpu().detach().numpy().transpose(1, 2, 0)\n",
    "    \n",
    "    perturbation = (perturbation * 255).astype(np.uint8)\n",
    "    adversarial_image = (adversarial_image * 255).astype(np.uint8)\n",
    "    original_image = (original_image * 255).astype(np.uint8)\n",
    "    \n",
    "    return original_image, perturbation, adversarial_image\n",
    "\n",
    "# Load and preprocess image\n",
    "image_path = 'yeongmin.jpeg'\n",
    "image_pil = Image.open(image_path).convert('RGB')\n",
    "image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "# Target text token (to maximize similarity with adversarial noise)\n",
    "target_text = \"blurred image\"\n",
    "target_text_token = clip.tokenize([target_text]).to(device)\n",
    "\n",
    "# Generate adversarial noise\n",
    "adversarial_image_tensor = generate_adversarial_noise(image_tensor, model, target_text_token, epsilon=0.03, steps=10)\n",
    "\n",
    "# Visualize the results\n",
    "original_image, perturbation_image, adversarial_image = visualize_perturbation(image_tensor.squeeze(0), adversarial_image_tensor.squeeze(0))\n",
    "\n",
    "# Save and display images\n",
    "cv2.imwrite('noised.jpg', original_image)\n",
    "cv2.imwrite('perturbation_image.jpg', perturbation_image)\n",
    "cv2.imwrite('adversarial_image.jpg', adversarial_image)\n",
    "\n",
    "# Display images\n",
    "Image.fromarray(original_image).show()\n",
    "Image.fromarray(perturbation_image).show()\n",
    "Image.fromarray(adversarial_image).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bdfac52-3142-4686-a4e8-9ca1071cd45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Image Classification Probabilities:\n",
      "a photo of a person: 99.51%\n",
      "blurred eyes: 0.41%\n",
      "blurred nose: 0.03%\n",
      "blurred mouth: 0.05%\n",
      "Noisy Image Classification Probabilities:\n",
      "a photo of a person: 99.37%\n",
      "blurred eyes: 0.54%\n",
      "blurred nose: 0.03%\n",
      "blurred mouth: 0.05%\n",
      "Difference Image Classification Probabilities:\n",
      "a photo of a person: 99.51%\n",
      "blurred eyes: 0.40%\n",
      "blurred nose: 0.01%\n",
      "blurred mouth: 0.08%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load CLIP model and preprocess function\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "\n",
    "# Define the text descriptions you want to classify with\n",
    "texts = [\"a photo of a person\", \"blurred eyes\", \"blurred nose\", \"blurred mouth\"]\n",
    "text_tokens = clip.tokenize(texts).to(device)\n",
    "\n",
    "def compute_image_difference(original_image_path, noisy_image_path, output_path):\n",
    "    # Load the original and noisy images\n",
    "    original_image = cv2.imread(original_image_path)\n",
    "    noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "    # Compute the absolute difference between the images\n",
    "    difference_image = cv2.absdiff(original_image, noisy_image)\n",
    "\n",
    "    # Save the difference image\n",
    "    cv2.imwrite(output_path, difference_image)\n",
    "\n",
    "    # Return the difference image for further analysis\n",
    "    return difference_image\n",
    "\n",
    "def classify_image(image, model, text_tokens):\n",
    "    # Preprocess the image\n",
    "    image_pil = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image_tensor = preprocess(image_pil).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode the image and texts using CLIP\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image_tensor)\n",
    "        text_features = model.encode_text(text_tokens)\n",
    "\n",
    "    # Calculate the similarity between the image and each text prompt\n",
    "    similarities = (image_features @ text_features.T).squeeze(0)\n",
    "    \n",
    "    # Normalize similarities to get probabilities (optional)\n",
    "    probs = similarities.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "    return probs\n",
    "\n",
    "def print_classification_results(probs, descriptions):\n",
    "    # Print each text description with its corresponding probability\n",
    "    for i, prob in enumerate(probs):\n",
    "        print(f\"{descriptions[i]}: {prob * 100:.2f}%\")\n",
    "\n",
    "# Step 1: Compute the difference image\n",
    "original_image_path = 'noised.jpg'  # Path to the original image\n",
    "noisy_image_path = 'perturbation_image.jpg'  # Path to the combined noisy image\n",
    "difference_image_path = 'difference_image.jpg'  # Path to save the difference image\n",
    "\n",
    "difference_image = compute_image_difference(original_image_path, noisy_image_path, difference_image_path)\n",
    "\n",
    "# Step 2: Classify the original, noisy, and difference images\n",
    "original_image = cv2.imread(original_image_path)\n",
    "noisy_image = cv2.imread(noisy_image_path)\n",
    "\n",
    "# Classify the original image\n",
    "original_probs = classify_image(original_image, model, text_tokens)\n",
    "print(\"Original Image Classification Probabilities:\")\n",
    "print_classification_results(original_probs, texts)\n",
    "\n",
    "# Classify the noisy image\n",
    "noisy_probs = classify_image(noisy_image, model, text_tokens)\n",
    "print(\"Noisy Image Classification Probabilities:\")\n",
    "print_classification_results(noisy_probs, texts)\n",
    "\n",
    "# Optional: Classify the difference image to see how the noise is perceived\n",
    "difference_probs = classify_image(difference_image, model, text_tokens)\n",
    "print(\"Difference Image Classification Probabilities:\")\n",
    "print_classification_results(difference_probs, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1962c51d-65aa-41be-97ea-c30f75a35839",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
